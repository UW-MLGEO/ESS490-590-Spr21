{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "_Warning!_ Although it is called logistic _regression_, logistic regression is actually a _classification_ method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we are going to talk about:\n",
    "- A simple classification method: Logistic regression\n",
    "- Gradient descent method\n",
    "- Automatic differentiation\n",
    "- Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember linear regression:\n",
    "\n",
    "$y = b + x w + \\epsilon$\n",
    "\n",
    "$y$ is a vector of length $n$, $x$ is a matrix with $n$ rows and $p$ columns, corresponding to $n$ observations and $p$ features that are used to explain $y$. \n",
    "\n",
    "$b$ is a scalar. $w$ is a vector of length $p$. $\\epsilon$ is a random error vector, of length $n$. It is independent of $x$, and has mean zero.\n",
    "\n",
    "Our objective is to find the best values of $b$ and $w$ so that the values of $\\hat{y} = b + x w$ are as close as possible to the actual values $y$.\n",
    "\n",
    "For linear regression, $y$ is a quantitative variable. What if $y$ is a qualitative variable, for example $y = 0$ for \"no\", and $y = 1$ for \"yes\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to use regression to solve a classification problem is to model the probability of the variable $y$ taking the value 1:\n",
    "\n",
    "$P (y = 1) = b + x w$\n",
    "\n",
    "Once we have found the best values $\\hat{b}$ and $\\hat{w}$, we compute $\\hat{y} = \\hat{b} + x \\hat{w}$. If $\\hat{y} \\geq 0.5$, we decide to classify this observation as $y = 1$, that is \"yes\". If $\\hat{y} < 0.5$, we decide to classify this observation as $y = 0$, that is \"no\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem with this method. We would like to have $0 \\leq P (y = 1) \\leq 1$ because it is a probability. However, there is nothing in this formulation that forces $b$ and $w$ to take values such that $\\hat{y} = \\hat{b} + x \\hat{w}$ will always takes values in $[0, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem, we can instead write:\n",
    "\n",
    "$z = b + x w$ and $P (y = 1) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "That way, we always have $0 \\leq P (y = 1) \\leq 1$. When $b + x w$ gets large, $P (y = 1)$ gets close to 1, and the value \"yes\" is more and more likely. When $b + x w$ gets small, $P (y = 1)$ gets close to 0, and the value \"no\" is more and more likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we find the optimal value of $b$ and $w$? We define the cross-entropy loss function. For one observation, the loss function is:\n",
    "\n",
    "$\\mathcal{L} = - (y_i \\log \\hat{y}_i + (1 - y_i) \\log (1 - \\hat{y}_i)$ with $\\hat{y}_i = \\frac{1}{1 + e^{- (b + x_i^T w)}}$ where $x_i$ is the $i$th row of x.\n",
    "\n",
    "If the true observation $y_i$ is 1 (\"yes\") and $\\hat{y}_i = 1$, the loss function takes the value 0. If $\\hat{y}_i = 0$, the loss function tends to infinity.\n",
    "\n",
    "If the true observation $y$ is 0 (\"no\") and $\\hat{y}_i = 0$, the loss function takes the value 0. If $\\hat{y}_i = 1$, the loss function tends to infinity.\n",
    "\n",
    "For all the $n$ observations, we write:\n",
    "\n",
    "$\\mathcal{L} = \\sum_{i = 1}^n \\mathcal{L}_i$\n",
    "\n",
    "Our objective is thus to find the values of $b$ and $\\omega$ that minimize the loss function. Note that with this formulation, $\\mathcal{L}$ is always positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "We know that the gradient $\\frac{\\partial \\mathcal{L}}{\\partial w_j}$ is positive if the loss $\\mathcal{L}$ increases when $w_j$ increases. Reversely, the gradient $\\frac{\\partial \\mathcal{L}}{\\partial w_j}$ is negative if the loss $\\mathcal{L}$ decreases when $w_j$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain smaller and smaller values of the loss, at each iteration we take:\n",
    "\n",
    "$w_j^{(k + 1)} = w_j^{(k)} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w_j}$ for $j = 1 , \\cdots , p$\n",
    "\n",
    "$b^{(k + 1)} = b^{(k)} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$\n",
    "\n",
    "We assume that the value of $\\alpha$ is not too big. If the gradient is positive, then the value of $w_j$ will decrease at each iteration, and the value of the loss function will decrease. If the gradient is negative, then the value of $w_j$ will increase at each iteration, and the value of the loss will decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, all we need to do is to compute the gradient of the loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three ways of computing the gradient. The first method is to use the formula of the loss:\n",
    "\n",
    "$\\mathcal{L} (w_j , b) = - \\sum_{i = 1}^n y_i \\log (\\frac{1}{1 + \\exp (- b - \\sum_{j = 1}^p w_j x_{i,j})}) + (1 - y_i) \\log (1 - \\frac{1}{1 + \\exp (- b - \\sum_{j = 1}^p w_j x_{i,j})})$\n",
    "\n",
    "and to calculate the exact formula of the derivatives $\\frac{\\partial \\mathcal{L}}{\\partial w_j}$ and $\\frac{\\partial \\mathcal{L}}{\\partial b}$. You just then have to implement the exact formula in the code to compute the gradient.\n",
    "\n",
    "When the formula gets more and more complicated, you become more and more likely to make a mistake, either in the calculation of the derivative formula, either in the implementation in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method is to compute an approximation of the gradient:\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\frac{\\mathcal{L}(w_j + \\Delta w_j) - \\mathcal{L}(w_j)}{\\Delta W_j}$\n",
    "\n",
    "If you write too many approximations, the method may not work very well and give inexact results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third method is to use automatic differentiation. If we write:\n",
    "\n",
    "$z = x_i^T w + b = f_x(w, b)$, $\\sigma = \\frac{1}{1 + e^{-z}} = g(z)$ and $L = - (y_i \\log(\\sigma) + (1 - y_i) \\log(1 - \\sigma)) = h_y(\\sigma)$, we get:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_j} = \\frac{\\partial f}{\\partial w_j} g'(z) h'(\\sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very easy to compute the exact formula of the derivatives:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial w_j}(w, b) = x_{i,j}$\n",
    "\n",
    "$g'(z) = \\frac{e^{-z}}{(1 + e^{-z})^2}$\n",
    "\n",
    "$h'(\\sigma) = - \\frac{y_i}{\\sigma} + \\frac{1 - y_i}{1 - \\sigma}$\n",
    "\n",
    "When computing $L$, we thus need to keep in memory the values of $\\frac{\\partial f}{\\partial w_j}(w, b)$, $g'(z)$, and $h'(\\sigma)$ to be able to compute the gradient. That is what PyTorch is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch (https://pytorch.org/) is a Python package which allows you to build and train neural networks. It is based on automatic differentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import exp\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import a dataset as an example. This example has been downloaded from Kaggle: https://www.kaggle.com/adityakadiwal/water-potability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('water_potability.csv')\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "x = data.drop(columns=['Potability']).to_numpy()\n",
    "y = data.Potability.to_numpy()\n",
    "N = len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to nomalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(x)\n",
    "x = scaler.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to compute the loss corresponding to the first observation in the dataset. Instead of using Numpy arrays to put our data and parameters, we are going to use torch tensors, because they have properties that Numpy arrays do not have.\n",
    "\n",
    "This is the features of the first observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_i = torch.torch.from_numpy(x[0, :])\n",
    "x_i = x_i.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the class of the first observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_i = y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take random values for $w$ and $b$. When creating these variables, we use the option requires_grad=True because we will later want to compute the gradient with respect to these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.rand(9, requires_grad=True)\n",
    "B = torch.rand(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be used to sepcify that we will want to compute the gradient with respect to the variable var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_grad(var):\n",
    "    def hook(grad):\n",
    "        var.grad = grad\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define $z = f(w, b) = x_i^T w + b$. We have $\\frac{\\partial f}{\\partial w_j} = x_{i,j}$ and $\\frac{\\partial f}{\\partial b} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = W.dot(x_i) + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x126f0e7f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.register_hook(set_grad(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define $\\sigma = g(z) = \\frac{1}{1 + e^{-z}} = g(f(w, b)) = (g \\circ f) (w, b)$. We have $g'(z) = \\frac{e^{-z}}{(1 + e^{-z})^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1.0 / (1.0 + torch.exp(- z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we use the function torch.exp instead of numpy.exp. That is because numpy just calculate the value of $e^x$ but does not know that the derivative of $e^x$ is $e^x$. If we want to be able to use automatic differentiation, we need to use the equivalent torch function that will compute both $\\sigma(z)$ and $\\frac{\\partial \\sigma}{\\partial z}(z)$. This last value will be necessary and we will later compute the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x1060f17f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma.register_hook(set_grad(sigma))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define $L = h(\\sigma) = - (y_i \\log(\\sigma) + (1 - y_i) \\log(1 - \\sigma)) = h(g(z)) = h(g(f(w, b))) = (h \\circ g \\circ f) (w, b)$. We have $L'(\\sigma) = - (\\frac{y_i}{\\sigma} - \\frac{1 - y_i}{1 - \\sigma})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = - (y_i * torch.log(sigma) + (1 - y_i) * torch.log(1 - sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the gradient of the loss for one observation. This command compute the gradient of L with respect to all the variables for which I asked to compute the gradient, that is W, B, z, and sigma, but it does not return the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $\\frac{\\partial L}{\\partial \\sigma} = - (\\frac{y_i}{\\sigma} - \\frac{1 - y_i}{1 - \\sigma})$. Let us compute the result with PyTorch and using the exact mathematical formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15.1225]) tensor([15.1225], grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(sigma.grad, - (y_i / sigma - (1 - y_i) / (1 - sigma)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $\\frac{\\partial L}{\\partial z} = g'(z) h'(g(z))$ that is $\\frac{\\partial L}{\\partial z} = g'(z) h'(\\sigma)$. Let us compute the result with PyTorch and using the exact mathematical formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9339]) tensor([0.9339], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(z.grad, (exp(-z) / ((1 + exp(-z)) ** 2.0)) * (- (y_i / sigma - (1 - y_i) / (1 - sigma))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $\\frac{\\partial L}{\\partial b} = \\frac{\\partial f}{\\partial b} g'(f(w, b)) h'(g(f(w, b)))$ that is $\\frac{\\partial L}{\\partial b} = \\frac{\\partial f}{\\partial b} g'(z) h'(\\sigma)$. Similarly, we have $\\frac{\\partial L}{\\partial w_j} = \\frac{\\partial f}{\\partial w_j} g'(f(w, b)) h'(g(f(w, b)))$ that is $\\frac{\\partial L}{\\partial w_j} = \\frac{\\partial f}{\\partial w_j} g'(z) h'(\\sigma)$. Let us compute the result with PyTorch and using the exact mathematical formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9339]) tensor([0.9339], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(B.grad, 1 * (exp(-z) / ((1 + exp(-z)) ** 2.0)) * (- (y_i / sigma - (1 - y_i) / (1 - sigma))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7307,  0.5268,  0.0109,  0.5452,  0.5364, -0.7321,  1.1459,  1.9720,\n",
      "         0.7889]) tensor([ 0.7307,  0.5268,  0.0109,  0.5452,  0.5364, -0.7321,  1.1459,  1.9720,\n",
      "         0.7889], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(W.grad, x_i * (exp(-z) / ((1 + exp(-z)) ** 2.0)) * (- (y_i / sigma - (1 - y_i) / (1 - sigma))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now implement logistic regression using the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.torch.from_numpy(x)\n",
    "X = X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.torch.from_numpy(y)\n",
    "Y = Y.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code for one iteration of the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(X, Y, W, B, alpha):\n",
    "    # Compute the loss for the current value of W and B\n",
    "    z = X @ W + B\n",
    "    sigma = 1.0 / (1.0 + torch.exp(- z))\n",
    "    L = torch.sum(- (Y * torch.log(sigma) + (1 - Y) * torch.log(1 - sigma)))\n",
    "    # Compute the gradient of the loss\n",
    "    L.backward()\n",
    "    # Specifically, we want the gradient with respect to W and B\n",
    "    dW = W.grad\n",
    "    dB = B.grad\n",
    "    # Update the values of W and B\n",
    "    W = W - alpha * dW\n",
    "    W.retain_grad()\n",
    "    B = B - alpha * dB\n",
    "    B.retain_grad()\n",
    "    # Return the new values of W and B\n",
    "    return (W, B, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement the logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, Y, alpha, max_iter, epsilon):\n",
    "    p = X.size()[1]\n",
    "    # We initiate W and B with random values\n",
    "    W = torch.rand(p, requires_grad=True)\n",
    "    B = torch.rand(1, requires_grad=True)\n",
    "    i_iter = 0\n",
    "    dL = 2 * epsilon\n",
    "    # We iterate until we reach the maximum number of iterations or the loss no longer decreases\n",
    "    while ((i_iter < max_iter) and (dL > epsilon)):\n",
    "        if i_iter > 0:\n",
    "            L_old = L\n",
    "        (W, B, L) = step(X, Y, W, B, alpha)\n",
    "        if i_iter > 0:\n",
    "            dL = abs((L - L_old) / L_old)\n",
    "        i_iter = i_iter + 1\n",
    "    return (W, B, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now run our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(W, B, L) = logistic_regression(X, Y, 0.001, 200, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try to make predictions on the training test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = 1.0 / (1.0 + torch.exp(- (X @ W + B)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the torch tensor to a Numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = yhat.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the values of the probability (between 0 and 1) into the value of the class to which each observation belongs (here 0 or 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.where(yhat > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute some classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# True positive\n",
    "tp = np.sum((y == 1) & (yhat == 1))\n",
    "print(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "802\n"
     ]
    }
   ],
   "source": [
    "# False negative\n",
    "fn = np.sum((y == 1) & (yhat == 0))\n",
    "print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# False positive\n",
    "fp = np.sum((y == 0) & (yhat == 1))\n",
    "print(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198\n"
     ]
    }
   ],
   "source": [
    "# True negative\n",
    "tn = np.sum((y == 0) & (yhat == 0))\n",
    "print(tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6001989060169071\n"
     ]
    }
   ],
   "source": [
    "# Accurracy (percentage of correct classifications)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011097410604192354\n"
     ]
    }
   ],
   "source": [
    "# Recall (= Sensitivity = percentage of positive value correctly classified)\n",
    "recall = tp / (tp + fn)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "# Precision (= percentage of positive predictions that were correct)\n",
    "precision = tp / (tp + fp)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021897810218978103\n"
     ]
    }
   ],
   "source": [
    "# F1\n",
    "F1 = (2 * precision * recall) / (precision + recall)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "Logistic regression is a nice example to start learning about automatic differentiation and PyTorch. However, if you actually want to use logistic regression for your own dataset, it is much easier to use the function already existing in ScikitLearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=0).fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04101425, -0.00117745,  0.08484803,  0.0454577 , -0.01693117,\n",
       "        -0.03103009, -0.02956544,  0.01923843,  0.04572756]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.39322864])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = precision_recall_fscore_support(y, yhat, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666 0.004932182490752158 0.009791921664626684\n"
     ]
    }
   ],
   "source": [
    "(precision, recall, F1) = (metrics[0], metrics[1], metrics[2])\n",
    "print(precision, recall, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
